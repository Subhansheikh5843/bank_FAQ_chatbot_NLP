{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/BankFAQs.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df['Question']\n",
    "answers = df['Answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def clean_text(text):\n",
    "  text = text.lower()\n",
    "  text = BeautifulSoup(text,'lxml').text\n",
    "  text = contractions.fix(text)\n",
    "  text = re.sub(r'[^A-Za-z\\s]','',text)\n",
    "  \n",
    "  text = re.sub(r'http\\S+|https\\S+|www\\S+','',text, flags=re.MULTILINE)\n",
    "  text = re.sub(r'\\S+@\\S+','',text)\n",
    "  text = re.sub('\\s+','  ',text).strip()\n",
    "  words = word_tokenize(text)\n",
    "  words = [lemmatizer.lemmatize(w)  for w in words if w not in stop_words]\n",
    "  text = ' '.join(words)\n",
    "  return text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [clean_text(i) for i in questions]\n",
    "answers = [clean_text(i) for i in  answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2count = {}\n",
    "for line in questions:\n",
    "  for word in line.split():\n",
    "    if word not in word2count:\n",
    "      word2count[word] = 1 \n",
    "    else:\n",
    "      word2count[word] +=1 \n",
    "      \n",
    "for line in answers:\n",
    "  for word in line.split():\n",
    "    if word not in word2count:\n",
    "      word2count[word] = 1 \n",
    "    else:\n",
    "      word2count[word] +=1 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "thresh = 5 \n",
    "a = 0\n",
    "for val,count in word2count.items():\n",
    "  if count >=5:\n",
    "    vocab[val] = a\n",
    "    a +=1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_in_len = max(len(w.split()) for w in questions)\n",
    "max_tr_len = max(len(w.split()) for w in answers)\n",
    "\n",
    "print(max_in_len)\n",
    "print(max_tr_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(answers)):\n",
    "    answers[i] = ' '.join(answers[i].split()[:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_in_len = max(len(w.split()) for w in questions)\n",
    "max_tr_len = max(len(w.split()) for w in answers)\n",
    "\n",
    "print(max_in_len)\n",
    "print(max_tr_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1/0\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(answers)):\n",
    "  answers[i] = '<SOS> ' + answers[i] + ' <EOS>' \n",
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_in_len = max(len(w.split()) for w in questions)\n",
    "max_tr_len = max(len(w.split()) for w in answers)\n",
    "\n",
    "print(max_in_len)\n",
    "print(max_tr_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['<PAD>','<SOS>','<OUT>','<EOS>']\n",
    "x = len(vocab)\n",
    "for token in tokens:\n",
    "  vocab[token] = x \n",
    "  x +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab['document'] = vocab['<PAD>']\n",
    "vocab['<PAD>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_vocab = {w:v for v,w in vocab.items()}\n",
    "inv_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inp = []\n",
    "for i in questions:\n",
    "  lst = []\n",
    "  for w in i.split():\n",
    "    if w not in vocab:\n",
    "      lst.append(vocab['<OUT>'])\n",
    "    else:\n",
    "      lst.append(vocab[w])\n",
    "  encoder_inp.append(lst)\n",
    "      \n",
    "decoder_inp = []\n",
    "for i in answers:\n",
    "  lst = []\n",
    "  for w in i.split():\n",
    "    if w not in vocab:\n",
    "      lst.append(vocab['<OUT>'])\n",
    "    else:\n",
    "      lst.append(vocab[w])\n",
    "      \n",
    "  decoder_inp.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_inp\n",
    "decoder_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "encoder_inp = pad_sequences(encoder_inp,max_in_len,padding='post',truncating='post')\n",
    "decoder_inp = pad_sequences(decoder_inp,max_tr_len,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inp.shape\n",
    "# decoder_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------remove sos from decoder inp----------------------------\n",
    "# Decoder Input: It is the input sequence fed into the decoder at each time step to predict the next token.\n",
    "\n",
    "# To align the decoder input with the target sequence for training, the <sos> token is included at the beginning of the decoder input, but it is not needed in the decoder target sequence because the target sequence is supposed to predict the next tokens without including the <sos> itself.\n",
    "decoder_final_output = []\n",
    "for i in decoder_inp:\n",
    "  decoder_final_output.append(i[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_final_output = pad_sequences(decoder_final_output,max_tr_len,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical \n",
    "decoder_final_output = to_categorical(decoder_final_output,len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------explantion of blw model  ----------------------\n",
    "\n",
    "# Step-by-Step Explanation\n",
    "# Input Layer for Encoder and Decoder:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# enc_inp = Input(shape=(13,))\n",
    "# dec_inp = Input(shape=(13,))\n",
    "# enc_inp and dec_inp are placeholders for input sequences for the encoder and decoder, respectively. Both have a fixed length of 13 tokens.\n",
    "# Embedding Layer:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# VOCAB_SIZE = len(vocab)\n",
    "# embed = Embedding(VOCAB_SIZE+1, output_dim=50, \n",
    "#                   input_length=13,\n",
    "#                   trainable=True)\n",
    "# VOCAB_SIZE is the number of unique words in your vocabulary.\n",
    "# Embedding layer transforms input tokens into dense vectors of fixed size (50 in this case). It adds 1 to the vocabulary size to account for special tokens.\n",
    "# input_length=13 means each input sequence has 13 tokens.\n",
    "# Encoder Embedding:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# enc_embed = embed(enc_inp)\n",
    "# The encoder input enc_inp is passed through the embedding layer, converting token sequences into sequences of vectors.\n",
    "# Encoder LSTM:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# enc_lstm = LSTM(400, return_sequences=True, return_state=True)\n",
    "# enc_op, h, c = enc_lstm(enc_embed)\n",
    "# enc_states = [h, c]\n",
    "# enc_lstm is an LSTM layer with 400 units.\n",
    "# return_sequences=True means it returns the entire output sequence.\n",
    "# return_state=True means it returns the last hidden state (h) and cell state (c).\n",
    "# enc_op is the output sequence from the LSTM.\n",
    "# h and c are the final hidden and cell states, respectively.\n",
    "# enc_states = [h, c] will be used to initialize the decoder's LSTM.\n",
    "# Decoder Embedding:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# dec_embed = embed(dec_inp)\n",
    "# The decoder input dec_inp is also passed through the same embedding layer, converting token sequences into sequences of vectors.\n",
    "# Decoder LSTM:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# dec_lstm = LSTM(400, return_sequences=True, return_state=True)\n",
    "# dec_op, _, _ = dec_lstm(dec_embed, initial_state=enc_states)\n",
    "# dec_lstm is another LSTM layer with 400 units.\n",
    "# dec_op is the output sequence from the LSTM.\n",
    "# The decoder's LSTM is initialized with the encoder's final states (enc_states).\n",
    "# Dense Layer:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# dense = Dense(VOCAB_SIZE, activation='softmax')\n",
    "# Dense layer with VOCAB_SIZE units and softmax activation to predict the next word in the sequence for each time step.\n",
    "# Output from Dense Layer:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# dense_op = dense(dec_op)\n",
    "# The output of the decoder LSTM (dec_op) is passed through the dense layer to get the final predictions.\n",
    "# Define the Model:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# model = Model([enc_inp, dec_inp], dense_op)\n",
    "# Define the model with inputs as [enc_inp, dec_inp] and output as dense_op.\n",
    "# Compile the Model:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "# Compile the model with categorical cross-entropy loss, accuracy metric, and Adam optimizer.\n",
    "# Train the Model:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# model.fit([encoder_inp, decoder_inp], decoder_final_output, epochs=15)\n",
    "# Train the model on the training data for 15 epochs. encoder_inp and decoder_inp are the input sequences, and decoder_final_output is the expected output sequences for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input,Embedding,LSTM,Dense\n",
    "from keras.models import Model \n",
    "VOCAB_SIZE = len(vocab)\n",
    "enc_inp = Input(shape=(16,))\n",
    "dec_inp = Input(shape=(16,))\n",
    "\n",
    "\n",
    "embed = Embedding(VOCAB_SIZE+1,output_dim = 50,input_length=16,trainable=True)\n",
    "enc_embed = embed(enc_inp)\n",
    "enc_lstm = LSTM(400,return_sequences=True,return_state=True)\n",
    "enc_op,h,c = enc_lstm(enc_embed)\n",
    "\n",
    "en_states = [h,c]\n",
    "\n",
    "dec_embed = embed(dec_inp)\n",
    "dec_lstm = LSTM(400,return_sequences=True,return_state=True)\n",
    "dec_op,_,_ = dec_lstm(dec_embed,initial_state=en_states)\n",
    "\n",
    "dense = Dense(VOCAB_SIZE,activation='softmax')\n",
    "dense_op = dense(dec_op)\n",
    "\n",
    "model = Model([enc_inp,dec_inp],dense_op)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "model.fit([encoder_inp, decoder_inp],decoder_final_output,epochs=70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input \n",
    "\n",
    "enc_model = Model([enc_inp],en_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(400,))\n",
    "decoder_state_input_c = Input(shape=(400,))\n",
    "decoder_state_initial_input = [decoder_state_input_h,decoder_state_input_c]\n",
    "\n",
    "decoder_outputs,state_h,state_c = dec_lstm(dec_embed,initial_state=decoder_state_initial_input)\n",
    "\n",
    "decoder_states = [state_h,state_c]\n",
    "\n",
    "\n",
    "dec_model = Model([dec_inp]+decoder_state_initial_input,[decoder_outputs]+decoder_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "print(\"##########################################\")\n",
    "print(\"#       start chatting ver. 1.0          #\")\n",
    "print(\"##########################################\")\n",
    "\n",
    "\n",
    "prepro1 = \"\"\n",
    "while prepro1 != 'q':\n",
    "    prepro1  = input(\"you : \")\n",
    "    ## prepro1 = \"Hello\"\n",
    "\n",
    "    prepro1 = clean_text(prepro1)\n",
    "    ## prepro1 = \"hello\"\n",
    "\n",
    "    prepro = [prepro1]\n",
    "    ## prepro1 = [\"hello\"]\n",
    "\n",
    "    txt = []\n",
    "    for x in prepro:\n",
    "        # x = \"hello\"\n",
    "        lst = []\n",
    "        for y in x.split():\n",
    "            ## y = \"hello\"\n",
    "            try:\n",
    "                lst.append(vocab[y])\n",
    "                ## vocab['hello'] = 454\n",
    "            except:\n",
    "                lst.append(vocab['<OUT>'])\n",
    "        txt.append(lst)\n",
    "\n",
    "    ## txt = [[454]]\n",
    "    txt = pad_sequences(txt, 16, padding='post')\n",
    "\n",
    "    ## txt = [[454,0,0,0,.........13]]\n",
    "\n",
    "    stat = enc_model.predict( txt )\n",
    "\n",
    "    empty_target_seq = np.zeros( ( 1 , 1) )\n",
    "     ##   empty_target_seq = [0]\n",
    "\n",
    "\n",
    "    empty_target_seq[0, 0] = vocab['<SOS>']\n",
    "    ##    empty_target_seq = [255]\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "\n",
    "    while not stop_condition :\n",
    "\n",
    "        dec_outputs , h, c= dec_model.predict([ empty_target_seq] + stat )\n",
    "        decoder_concat_input = dense(dec_outputs)\n",
    "        \n",
    "        ##decoder_concat_input[0, -1, :] gives [0.1, 0.2, 0.4, 0.3, ...], which are the probabilities for each word in the vocabulary at the current time step.\n",
    "\n",
    "        sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )\n",
    "        ## sampled_word_index = [2]\n",
    "\n",
    "        sampled_word = inv_vocab[sampled_word_index] + ' '\n",
    "\n",
    "        ## inv_vocab[2] = 'hi'\n",
    "        ## sampled_word = 'hi '\n",
    "\n",
    "        if sampled_word != '<EOS> ':\n",
    "            decoded_translation += sampled_word  \n",
    "\n",
    "        if sampled_word == '<EOS> ' or len(decoded_translation.split()) > 16:\n",
    "            stop_condition = True \n",
    "\n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        ## <SOS> - > hi\n",
    "        ## hi --> <EOS>\n",
    "        stat = [h, c]  \n",
    "\n",
    "    print(\"chatbot attention : \", decoded_translation )\n",
    "    print(\"==============================================\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions\n",
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
